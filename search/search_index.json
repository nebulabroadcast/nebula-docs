{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>hello world</p>"},{"location":"basics/architecture/","title":"Architecture","text":""},{"location":"basics/architecture/#site","title":"Site","text":"<p>A Nebula instance. A site may be a broadcaster or broadcast network with one or more channels sharing the configuration, subset of assets, or users. Each site has its own database, configuration template, and users.</p> <p>One site can be run on multiple servers to spread the load.</p> <p>The heart of each site is a PostgreSQL, Redis and a shared storage (or multiple storages). Nebula server and workers all access these resources.</p>"},{"location":"basics/architecture/#server","title":"Server","text":"<p>Nebula server provides API and the web interface for Nebula. Server needs to have access to Postgres and Redis, as well as the storage, that is used to store low-res media and uploads.</p>"},{"location":"basics/architecture/#worker","title":"Worker","text":"<p>Nebula worker is a program that starts Nebula services, which are small programs designed to perform specific tasks.</p> <p>Some services extract metadata from media files, others transcode assets to different formats, control playout servers, and more. These services can be distributed across multiple machines within the network and managed through the Nebula web interface. Nebula is highly scalable, allowing each site to use workers to distribute media processing tasks across the network.</p> <p>Additionally, it is possible to develop custom, site-specific services for very specialized tasks.</p>"},{"location":"basics/asset-management/","title":"Asset management","text":""},{"location":"basics/asset-management/#assets-and-folders","title":"Assets and folders","text":"<p>An asset is a database entry that can receive an unlimited amount of characteristics, such as associated media files, metadata, etc.</p>"},{"location":"basics/asset-management/#folders","title":"Folders","text":"<p>Assets, that share the same default characteristics are of the same folder, such as movie, episode, song, story, jingle, etc.</p> <p>Folder selection also determines, which metadata keys are displayed in the asset editor.</p>"},{"location":"basics/asset-management/#content-type","title":"Content type","text":"<p>Content-type is an asset attribute defining what type of content asset represents. Following content types are available:</p> <p>AUDIO VIDEO IMAGE TEXT DATABROADCASTING INTERSTITIAL EDUCATION APPLICATION GAME PACKAGE</p>"},{"location":"basics/asset-management/#media-type","title":"Media type","text":"<p>Assets may not be necessarily linked to media files. Special assets such as series, have media type virtual. These assets are just database records with metadata attached. Following media types are available:</p> <p>VIRTUAL FILE URI</p>"},{"location":"basics/asset-management/#view","title":"View","text":"<p>Views serve as the main filter of assets in the browser. Each view can be configured to contain assets from specific folders, with the given status and matching certain conditions. Additionally, each view have a default set of metadata displayed as columns in the browser table.</p> <p>Within the view, you can sort assets by the displayed columns and filter further using full-text search or advanced search queries.</p>"},{"location":"basics/asset-management/#actions-and-jobs","title":"Actions and jobs","text":"<p>Action is a predefined process such as transcoding an asset to a different format or transfer to remote storage. Each action has certain conditions, which an asset must match to start the action automatically or to allow manual triggering. When triggered, a job is created. A job is therefore defined by an asset, an action, and custom settings.</p> <p>You can monitor the status of a job and its and progress both in Firefly and the web interface.</p>"},{"location":"basics/linear-broadcasting/","title":"Scheduling and playout","text":""},{"location":"basics/linear-broadcasting/#channel","title":"Channel","text":"<p>Typically a playout channel. Each site can have several channels. In some cases such as a VOD application, Nebula may be configured without any channels.</p>"},{"location":"basics/linear-broadcasting/#event","title":"Event","text":"<p>A calendar entry. In the case of linear playout scheduling, an event is an \u201cEPG\u201d program block. An event belongs to a channel and has its start time and other metadata such as title and description.</p> <p>Users can view and edit events using the scheduler view in the Firefly application.</p>"},{"location":"basics/linear-broadcasting/#item","title":"Item","text":"<p>Generally, an item is an instance of an asset scheduled in an event playlist. Several special types of items (without relation to any asset) may be inserted to a playlist as well, such as live events, placeholders, etc.</p>"},{"location":"basics/linear-broadcasting/#rundown","title":"Rundown","text":"<p>List of all events and items for one day. A Firefly rundown module displays complete daily playlist in a single view.</p>"},{"location":"basics/linear-broadcasting/#as-run","title":"As-Run","text":"<p>An as-run log is the actual and accurate list of items which have been played out. Nebula provides a mechanism to filter and export the log to various formats for clients, collective rights management organizations, etc.</p>"},{"location":"installation/docker/","title":"Docker based installation","text":""},{"location":"setup/casparcg/","title":"CasparCG playout","text":""},{"location":"setup/casparcg/#controlling-casparcg-from-docker","title":"Controlling CasparCG from Docker","text":"<p>Should your <code>play</code> service run in Docker and use <code>casparcg</code> engine, you need to allow bi-directional communication between the service and the playout server. CasparCG uses OSC protocol to provide information about the current state of playback. When running in Docker, container has to expose an UDP port Caspar will connect to.</p> <p>Let's assume there are two <code>play</code> services running in a container, controlling two channels of the same CasparCG server.</p> <p>Each channel must be configured with unique <code>controller_port</code> and <code>caspar_osc_port</code> values in the site template:</p> <p>settings/channels.py</p> <pre><code>from nebula.settings.models import PlayoutChannelSettings, AcceptModel\n\nscheduler_accepts = AcceptModel(folders=[1, 2])\nrundown_accepts = AcceptModel(folders=[1, 3, 4, 5, 6, 7, 8, 9, 10])\n\nchannel1 = PlayoutChannelSettings(\n    id=1,\n    name=\"Channel 1\",\n    fps=25.0,\n    plugins=[],\n    solvers=[],\n    day_start=(7, 0),\n    scheduler_accepts=scheduler_accepts,\n    rundown_accepts=rundown_accepts,\n    rundown_columns=[],\n    send_action=2,\n    engine=\"casparcg\",\n    allow_remote=False,\n    controller_host=\"worker\",\n    controller_port=42101,\n    playout_storage=3,\n    playout_dir=\"media\",\n    playout_container=\"mxf\",\n    config={\n        \"caspar_host\": \"192.168.1.100\",\n        \"caspar_port\": 5250,\n        \"caspar_osc_port\": 6251,\n        \"caspar_channel\": 1,\n        \"caspar_feed_layer\": 10,\n    },\n)\n\n# Configure second channel similarly\n\nCHANNELS = [channel1, channel2]\n</code></pre> <p>Then setup both controllers services to run on the <code>worker</code> host:</p> <p>settings/services.py</p> <pre><code>PLAY1 = \"&lt;settings&gt;&lt;id_channel&gt;1&lt;/id_channel&gt;&lt;/settings&gt;\"\nPLAY2 = \"&lt;settings&gt;&lt;id_channel&gt;2&lt;/id_channel&gt;&lt;/settings&gt;\"\n\nSERVICES = [\n   # ...\n   ServiceSettings(id=11, type=\"play\", name=\"play1\", host=\"worker\", settings=PLAY1),  \n   ServiceSettings(id=12, type=\"play\", name=\"play2\", host=\"worker\", settings=PLAY2),\n   # ...\n]\n</code></pre> <p>And we create configuration files for both services:</p> <p>In the <code>docker-compose.yml</code> we setup UDP port forwarding for both ports we specified in the channels configuration to the host running the play services:</p> <pre><code>ports:\n  - \"6251:6251/udp\"\n  - \"6252:6252/udp\"\n</code></pre> <p>And last, in the <code>casparcg.config</code> file, we enable sending OSC messages to the host.</p> <pre><code>&lt;osc&gt;\n  &lt;predefined-clients&gt;\n    &lt;predefined-client&gt;\n      &lt;address&gt;IP_ADDRESS_OF_DOCKER_HOST&lt;/address&gt;\n      &lt;port&gt;6251&lt;/port&gt;\n    &lt;/predefined-client&gt;\n    &lt;predefined-client&gt;\n      &lt;address&gt;IP_ADDRESS_OF_DOCKER_HOST&lt;/address&gt;\n      &lt;port&gt;6252&lt;/port&gt;\n    &lt;/predefined-client&gt;\n  &lt;/predefined-clients&gt;\n&lt;/osc&gt;\n</code></pre>"},{"location":"setup/casparcg/#media","title":"Media","text":"<p>Our best practice is to have <code>media.dir</code> directory on the playout server data drive (e.g. <code>d:\\media.dir</code> on Windows ), then we share the media drive, so it's accesible as for example <code>\\\\playoutserver\\playout</code></p> <p>In <code>casparcg.config</code> set the <code>&lt;media-path&gt;</code> to the local path to the directory.</p>"},{"location":"setup/casparcg/#send-to-playout","title":"Send to playout","text":"<p>In order to copy media files to the playout storage, create an action for each physical playout storage.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;settings&gt;\n    &lt;allow_if&gt;True&lt;/allow_if&gt;\n    &lt;task mode=\"ffmpeg\"&gt;\n        &lt;param name=\"filter:a\"&gt;\"loudnorm=I=-23\"&lt;/param&gt;\n        &lt;param name=\"ar\"&gt;48000&lt;/param&gt;\n        &lt;param name=\"c:v\"&gt;\"copy\"&lt;/param&gt;\n        &lt;param name=\"c:a\"&gt;\"pcm_s16le\"&lt;/param&gt;\n        &lt;output storage=\"asset.get_playout_storage(1)\" direct=\"1\"&gt;&lt;![CDATA[asset.get_playout_path(1)]]&gt;&lt;/output&gt;\n    &lt;/task&gt;\n&lt;/settings&gt;\n</code></pre>"},{"location":"setup/casparcg/#psm-service","title":"PSM Service","text":"<p>PSM (Playout storage monitor) is a Nebula service which controls sending media filest to the playout storage automatically based on the schedule. By default it start the job 24 hours before broadcast time.</p> <p>Only one PSM instance is needed per installation as it handles all configured playout channels. No further configuration of the service is required.</p>"},{"location":"system/importing-media/","title":"Importing media","text":""},{"location":"system/importing-media/#watchfolders","title":"Watchfolders","text":"<p>The easiest way to import media files to nebula is to use watchfolders.  That way, new assets are created automatically for each file uploaded to  a defined directory. To use watchfolders, use a single instance of the <code>watch</code> service. Service configuration may contain one or more watchfolders.</p> <p><pre><code>&lt;service&gt;\n    &lt;folder id_storage=\"1\" path=\"media.dir/movies\"/&gt;\n    &lt;folder id_storage=\"2\" path=\"media.dir/episodes\"/&gt;\n&lt;/service&gt;\n</code></pre> Folder tag attributes:</p> <ul> <li><code>id_storage</code> (required)</li> <li><code>path</code> (required)</li> <li><code>id_folder</code> (default 12 - Incoming)</li> <li><code>recursive</code> (default True)</li> <li><code>hidden</code> (default False) - Ignore dotfiles</li> <li><code>quarantine_time</code> (default 10)</li> <li><code>case_sensitive_exts</code> (default False)</li> </ul> <p>It is also possible to execute a script to add/modify the metadata of the new asset:</p> <pre><code>&lt;service&gt;\n  &lt;folder id_storage=\"1\" path=\"media.dir\" recursive=\"1\" id_folder=\"1\"&gt;\n    &lt;post&gt;\n&lt;![CDATA[\nimport shortuuid\nasset[\"id/main\"] = shortuuid.uuid()\n]]&gt;\n    &lt;/post&gt;\n  &lt;/folder&gt;\n&lt;/service&gt;\n</code></pre> <p>If you are using watchfolders, a media file (quite obviously) must exist prior to the creation of the asset.</p>"},{"location":"system/importing-media/#manual-asset-creation","title":"Manual asset creation","text":"<p>Nebula also supports the opposite method: Create an asset first using the web interface  or Firefly and enforce the user to fill all required metadata before they are able to upload the file.  This can be done using an asset validation plugin.</p> <p>See <code>plugins/validator/asset.py</code> for an example.</p>"},{"location":"system/remote-storages/","title":"Remote storages","text":"<p>By default, nebula-example uses local storage on the server where it is installed (<code>storage</code> directory mounted directly to the docker container).  However, you can also configure Nebula to use network storage for your media files. </p> <p>In this tutorial, we will show you how to set up and configure network storage for Nebula.</p>"},{"location":"system/remote-storages/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, you should have the following:</p> <ul> <li>A network share that you want to use for Nebula storage</li> <li>Access to the server where Nebula is installed</li> <li>Basic knowledge of Docker and Docker Compose</li> </ul>"},{"location":"system/remote-storages/#updating-docker-compose","title":"Updating docker-compose","text":"<p>In the docker-compose.yml file, remove the volume definitions for both the backend and server services that point to the storages directory:</p> <pre><code>services:\n  backend:\n    volumes:\n      # Remove this line\n      - ./storages:/mnt/nebula_01\n\n  worker:\n    volumes:\n      # Remove this line\n      - ./storages:/mnt/nebula_01\n</code></pre> <p>Docker containers must be privileged in order to be allowed to mount samba storages: Add the following lines to their definition in the <code>docker-compose.yml</code></p> <pre><code>    cap_add:\n        - SYS_ADMIN\n        - DAC_READ_SEARCH\n    privileged: true\n</code></pre>"},{"location":"system/remote-storages/#define-a-remote-storage-in-nebula","title":"Define a remote storage in Nebula","text":"<p>In the settings directory of the Nebula repository, create a storages.py file with the following contents:</p> <pre><code>from nebula.settings.models import StorageSettings\n\nSTORAGES = [\n    StorageSettings(\n        id=1,\n        name=\"production\",\n        protocol=\"samba\",\n        path=\"//server/share\",\n        options={\n            \"login\": \"user\",\n            \"password\": \"password\",\n        },\n    )\n]\n</code></pre> <p>Start the stack again with <code>docker compose up</code></p>"}]}