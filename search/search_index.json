{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Nebula is an open-source media asset management (MAM) and broadcast automation system  engineered for demanding 24/7 broadcast environments.</p> <p>It serves as the central orchestrator for media assets, scheduling, and playout control.</p> <p>The system is built with an emphasis on simplicity, modularity, and rapid response,  leveraging open standards and APIs to facilitate integration into both existing and new broadcast workflows.</p>"},{"location":"basics/architecture/","title":"Architecture","text":""},{"location":"basics/architecture/#site","title":"Site","text":"<p>A Nebula instance. A site may be a broadcaster or broadcast network with one or more channels sharing the configuration, subset of assets, or users. Each site has its own database, configuration template, and users.</p> <p>One site can be run on multiple servers to spread the load.</p> <p>The heart of each site is a PostgreSQL, Redis and a shared storage (or multiple storages). Nebula server and workers all access these resources.</p>"},{"location":"basics/architecture/#server","title":"Server","text":"<p>Nebula server provides API and the web interface for Nebula. Server needs to have access to Postgres and Redis, as well as the storage, that is used to store low-res media and uploads.</p>"},{"location":"basics/architecture/#worker","title":"Worker","text":"<p>Nebula worker is a program that starts Nebula services, which are small programs designed to perform specific tasks.</p> <p>Some services extract metadata from media files, others transcode assets to different formats, control playout servers, and more. These services can be distributed across multiple machines within the network and managed through the Nebula web interface. Nebula is highly scalable, allowing each site to use workers to distribute media processing tasks across the network.</p> <p>Additionally, it is possible to develop custom, site-specific services for very specialized tasks.</p>"},{"location":"basics/asset-management/","title":"Asset management","text":""},{"location":"basics/asset-management/#assets-and-folders","title":"Assets and folders","text":"<p>An asset is a database entry that can receive an unlimited amount of characteristics, such as associated media files, metadata, etc.</p>"},{"location":"basics/asset-management/#folders","title":"Folders","text":"<p>Assets, that share the same default characteristics are of the same folder, such as movie, episode, song, story, jingle, etc.</p> <p>Folder selection also determines, which metadata keys are displayed in the asset editor.</p>"},{"location":"basics/asset-management/#content-type","title":"Content type","text":"<p>Content-type is an asset attribute defining what type of content asset represents. Following content types are available:</p> <p>AUDIO VIDEO IMAGE TEXT DATABROADCASTING INTERSTITIAL EDUCATION APPLICATION GAME PACKAGE</p>"},{"location":"basics/asset-management/#media-type","title":"Media type","text":"<p>Assets may not be necessarily linked to media files. Special assets such as series, have media type virtual. These assets are just database records with metadata attached. Following media types are available:</p> <p>VIRTUAL FILE URI</p>"},{"location":"basics/asset-management/#view","title":"View","text":"<p>Views serve as the main filter of assets in the browser. Each view can be configured to contain assets from specific folders, with the given status and matching certain conditions. Additionally, each view have a default set of metadata displayed as columns in the browser table.</p> <p>Within the view, you can sort assets by the displayed columns and filter further using full-text search or advanced search queries.</p>"},{"location":"basics/asset-management/#actions-and-jobs","title":"Actions and jobs","text":"<p>Action is a predefined process such as transcoding an asset to a different format or transfer to remote storage. Each action has certain conditions, which an asset must match to start the action automatically or to allow manual triggering. When triggered, a job is created. A job is therefore defined by an asset, an action, and custom settings.</p> <p>You can monitor the status of a job and its and progress both in Firefly and the web interface.</p>"},{"location":"basics/linear-broadcasting/","title":"Linear Broadcasting","text":""},{"location":"basics/linear-broadcasting/#channel","title":"Channel","text":"<p>Typically a playout channel. Each site can have several channels.  In some cases such as a VOD application, Nebula may be configured without any channels.</p>"},{"location":"basics/linear-broadcasting/#event","title":"Event","text":"<p>A calendar entry. In the case of linear playout scheduling, an event is an \"EPG\" program block.  An event belongs to a channel and has its start time and other metadata such as title and description.</p> <p>Users can view and edit events using the scheduler view in the Firefly application.</p>"},{"location":"basics/linear-broadcasting/#item","title":"Item","text":"<p>Generally, an item is an instance of an asset scheduled in an event playlist.  Several special types of items (without relation to any asset) may be inserted to a playlist as well, such as live events, placeholders, etc.</p>"},{"location":"basics/linear-broadcasting/#rundown","title":"Rundown","text":"<p>List of all events and items for one day. Rundown view shows all items scheduled for a channel on a given day.  It is used to create and edit the schedule, as well as to monitor the playout.</p>"},{"location":"basics/linear-broadcasting/#as-run","title":"As-Run","text":"<p>An as-run log is the actual and accurate list of items which have been played out.  Nebula provides a mechanism to filter and export the log to various formats for clients, collective rights management organizations, etc.</p>"},{"location":"installation/docker/","title":"Docker based installation","text":""},{"location":"setup/casparcg/","title":"CasparCG playout","text":"<p>CasparCG Server is a professional-grade, open-source software available for both Windows and Linux,  dedicated to playing out graphics, audio, and video to multiple outputs.</p> <p>Its long-standing track record, having been in 24/7 broadcast production since 2006,  attests to its stability and reliability in mission-critical environments.</p> <p>Nebula leverages this inherent robustness for its on-air operations,  ensuring that the playout layer is as dependable as the automation system itself. </p> <p>Furthermore, Nebula\u2019s playout control service is designed with a plug-in interface,  enabling the execution of secondary events such as character generation (CG),  router control, studio control, and recorder control.</p>"},{"location":"setup/casparcg/#prerequisites","title":"Prerequisites","text":""},{"location":"setup/casparcg/#controlling-casparcg-from-docker","title":"Controlling CasparCG from Docker","text":"<p>Should your <code>play</code> service run in Docker and use <code>casparcg</code> engine, you need to allow bi-directional communication between the service and the playout server. CasparCG uses OSC protocol to provide information about the current state of playback. When running in Docker, container has to expose an UDP port Caspar will connect to.</p> <p>Let's assume there are two <code>play</code> services running in a container, controlling two channels of the same CasparCG server.</p> <p>Each channel must be configured with unique <code>controller_port</code> and <code>caspar_osc_port</code> values in the site template:</p> <p>settings/channels.py</p> <pre><code>from nebula.settings.models import PlayoutChannelSettings, AcceptModel\n\nscheduler_accepts = AcceptModel(folders=[1, 2])\nrundown_accepts = AcceptModel(folders=[1, 3, 4, 5, 6, 7, 8, 9, 10])\n\nchannel1 = PlayoutChannelSettings(\n    id=1,\n    name=\"Channel 1\",\n    fps=25.0,\n    plugins=[],\n    solvers=[],\n    day_start=(7, 0),\n    scheduler_accepts=scheduler_accepts,\n    rundown_accepts=rundown_accepts,\n    rundown_columns=[],\n    send_action=2,\n    engine=\"casparcg\",\n    allow_remote=False,\n    controller_host=\"worker\",\n    controller_port=42101,\n    playout_storage=3,\n    playout_dir=\"media\",\n    playout_container=\"mxf\",\n    config={\n        \"caspar_host\": \"192.168.1.100\",\n        \"caspar_port\": 5250,\n        \"caspar_osc_port\": 6251,\n        \"caspar_channel\": 1,\n        \"caspar_feed_layer\": 10,\n    },\n)\n\n# Configure second channel similarly\n\nCHANNELS = [channel1, channel2]\n</code></pre> <p>Then setup both controllers services to run on the <code>worker</code> host:</p> <p>settings/services.py</p> <pre><code>PLAY1 = \"&lt;settings&gt;&lt;id_channel&gt;1&lt;/id_channel&gt;&lt;/settings&gt;\"\nPLAY2 = \"&lt;settings&gt;&lt;id_channel&gt;2&lt;/id_channel&gt;&lt;/settings&gt;\"\n\nSERVICES = [\n   # ...\n   ServiceSettings(id=11, type=\"play\", name=\"play1\", host=\"worker\", settings=PLAY1),  \n   ServiceSettings(id=12, type=\"play\", name=\"play2\", host=\"worker\", settings=PLAY2),\n   # ...\n]\n</code></pre> <p>And we create configuration files for both services:</p> <p>In the <code>docker-compose.yml</code> we setup UDP port forwarding for both ports we specified in the channels configuration to the host running the play services:</p> <pre><code>ports:\n  - \"6251:6251/udp\"\n  - \"6252:6252/udp\"\n</code></pre> <p>And last, in the <code>casparcg.config</code> file, we enable sending OSC messages to the host.</p> <pre><code>&lt;osc&gt;\n  &lt;predefined-clients&gt;\n    &lt;predefined-client&gt;\n      &lt;address&gt;IP_ADDRESS_OF_DOCKER_HOST&lt;/address&gt;\n      &lt;port&gt;6251&lt;/port&gt;\n    &lt;/predefined-client&gt;\n    &lt;predefined-client&gt;\n      &lt;address&gt;IP_ADDRESS_OF_DOCKER_HOST&lt;/address&gt;\n      &lt;port&gt;6252&lt;/port&gt;\n    &lt;/predefined-client&gt;\n  &lt;/predefined-clients&gt;\n&lt;/osc&gt;\n</code></pre>"},{"location":"setup/casparcg/#media","title":"Media","text":"<p>Our best practice is to have <code>media.dir</code> directory on the playout server data drive (e.g. <code>d:\\media.dir</code> on Windows ), then we share the media drive, so it's accesible as for example <code>\\\\playoutserver\\playout</code></p> <p>In <code>casparcg.config</code> set the <code>&lt;media-path&gt;</code> to the local path to the directory.</p>"},{"location":"setup/casparcg/#send-to-playout","title":"Send to playout","text":"<p>In order to copy media files to the playout storage, create an action for each physical playout storage.</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;settings&gt;\n    &lt;allow_if&gt;True&lt;/allow_if&gt;\n    &lt;task mode=\"ffmpeg\"&gt;\n        &lt;param name=\"filter:a\"&gt;\"loudnorm=I=-23\"&lt;/param&gt;\n        &lt;param name=\"ar\"&gt;48000&lt;/param&gt;\n        &lt;param name=\"c:v\"&gt;\"copy\"&lt;/param&gt;\n        &lt;param name=\"c:a\"&gt;\"pcm_s16le\"&lt;/param&gt;\n        &lt;output storage=\"asset.get_playout_storage(1)\" direct=\"1\"&gt;&lt;![CDATA[asset.get_playout_path(1)]]&gt;&lt;/output&gt;\n    &lt;/task&gt;\n&lt;/settings&gt;\n</code></pre>"},{"location":"setup/casparcg/#psm-service","title":"PSM Service","text":"<p>PSM (Playout storage monitor) is a Nebula service which controls sending media filest to the playout storage automatically based on the schedule. By default it start the job 24 hours before broadcast time.</p> <p>Only one PSM instance is needed per installation as it handles all configured playout channels. No further configuration of the service is required.</p>"},{"location":"setup/storages/","title":"Nebula storages","text":""},{"location":"setup/storages/#storage-types","title":"Storage Types","text":""},{"location":"setup/storages/#production-storage","title":"Production Storage","text":""},{"location":"setup/storages/#characteristics","title":"Characteristics","text":"<p>This environment requires high capacity to store large volumes of raw footage, master files, and intermediate assets.  It needs good read/write performance to handle concurrent ingest and transcoding jobs, but can tolerate slightly  higher latency compared to playout. </p> <p>Robust backup and archiving capabilities are also essential for long-term asset preservation.</p>"},{"location":"setup/storages/#implementation","title":"Implementation","text":"<p>Network Attached Storage (NAS) or Storage Area Network (SAN) solutions are commonly employed,  supporting standard network file protocols like NFS or Server Message Block (SMB)/Common Internet File System (CIFS).</p> <p>A hybrid approach combining spinning disks for capacity and SSDs for frequently accessed media or work-in-progress projects  can offer a balance of cost and performance.</p>"},{"location":"setup/storages/#playout-storage","title":"Playout Storage","text":""},{"location":"setup/storages/#characteristics_1","title":"Characteristics","text":"<p>This storage demands extremely high, consistent read performance with very low latency to ensure uninterrupted playback  and prevent dropped frames. </p> <p>Capacity requirements are typically lower than production storage, as it only needs to hold media for the active broadcast schedule  (e.g., a few days or weeks of content).</p>"},{"location":"setup/storages/#implementation_1","title":"Implementation","text":"<p>Direct Attached Storage (DAS) with high-speed Solid State Drives (SSDs) directly connected to the CasparCG server  is often preferred for maximum performance and lowest latency. </p> <p>Alternatively, a dedicated, high-performance volume on a SAN, specifically tuned for sequential reads, can be utilized.  Redundancy mechanisms like RAID 10 or mirroring are essential to ensure continuous operation in the event of drive failure.</p> <p>The table below summarizes the key differences and requirements for production and playout storage:</p> Feature Production Storage Playout Storage Purpose File ingest, editing, archiving, high-res masters Real-time playout, optimized delivery Capacity Large (terabytes to petabytes) Sufficient for active playout schedule (days/weeks) Performance High I/O for ingest/transcoding, moderate latency High read speeds, very low latency, consistent throughput Access Patterns Random read/write, large file transfers Sequential read, concurrent access by playout engines Redundancy High (RAID 6, distributed storage, backups) High (RAID 10, mirrored, fast failover) Connectivity SAN, NAS (NFS, SMB), Cloud Storage Direct Attached Storage (DAS), high-speed SAN/NAS File Formats Original camera formats, mezzanine codecs Playout-specific codecs (e.g., ProRes, DNxHD, H.264)"},{"location":"setup/storages/#configuration","title":"Configuration","text":"<p>Storages in Nebula are referenced by their ID (e.g., <code>1</code>, <code>2</code>, etc.) and storages are expected to be mounted in  <code>/mnt/{site_name}_{id}</code> directory of the node (e.g., <code>/mnt/nebula_01</code>). When a storage is configured using a settings template, its configuration is stored in the database and Nebula attempts to mount it on each node at startup (and keeps it mounted).</p> <p>All media files in Nebula are then referenced by their storage ID and path relative to the storage mount point.</p>"},{"location":"setup/storages/#local-storage","title":"Local storage","text":"<p>When a storage is configured as local, Nebula doesn't attempt to mount it on the node,  but rather expects it to be mounted at the specified path. Local storages don't need to be explicitly defined in the database.</p> <p>This is useful for single-node deployments, where a local directory is bind-mounted to the node's <code>/mnt/{site_name}_{id}</code> directory as a Docker volume or a Kubernetes Persistent Volume (PV).</p>"},{"location":"setup/storages/#configuration-example","title":"Configuration Example","text":"<p>In <code>docker-compose.yml</code>, you can define a local storage by adding a volume mount to the backend and worker services:</p> <pre><code>services:\n  backend:\n    volumes:\n      - ./nebula-storage:/mnt/nebula_01\n\n  worker:\n    volumes:\n      - ./nebula-storage:/mnt/nebula_01\n</code></pre>"},{"location":"setup/storages/#shared-storages","title":"Shared storages","text":"<p>Samba or NFS storages require a valid configuration in the settings template. </p> <p>Before you begin, you should have the following:</p> <ul> <li>A network share that you want to use for Nebula storage</li> <li>Access to the server where Nebula is installed</li> <li>Basic knowledge of Docker and Docker Compose</li> </ul>"},{"location":"setup/storages/#configuration-example_1","title":"Configuration Example","text":"<p>In the settings directory of the Nebula repository, create <code>storages.py</code> file with the following contents (modify the values as needed):</p> <pre><code>from nebula.settings.models import StorageSettings\n\nSTORAGES = [\n    StorageSettings(\n        id=1,\n        name=\"production\",\n        protocol=\"samba\",\n        path=\"//server/share\",\n        options={\n            \"login\": \"user\",\n            \"password\": \"password\",\n        },\n    )\n]\n</code></pre> <p>Then run <code>make setup</code> to apply the settings.</p>"},{"location":"setup/storages/#docker-considerations","title":"Docker considerations","text":"<p>In order to mount shared storages in Docker, the container must be configured to run in \"privileged mode\".  This setting grants the container extensive access to the host's resources, including the crucial ability to mount external volumes.</p> <p>Specifically, the <code>docker-compose.yml</code> configuration for Nebula server and nodes should include <code>privileged: true</code> directive under the service definition. </p> <pre><code>    cap_add:\n        - SYS_ADMIN\n        - DAC_READ_SEARCH\n    privileged: true\n</code></pre>"},{"location":"setup/watchfolders/","title":"Watch folders","text":"<p>Using watch folders is the most convenient way to import media files into Nebula.</p> <p>Watch folders allow you to automatically create assets in Nebula when new media files are added to specific directories. </p> <p>This eliminates the need for manual asset creation and ensures that your media library is always up-to-date.</p> <p>To use watch folders, use a single instance of the <code>watch</code> service.  Service configuration may contain one or more watch folders.</p>"},{"location":"setup/watchfolders/#minimal-configuration","title":"Minimal configuration","text":"<p>The following example shows a minimal configuration for a watch folder service, monitoring two directories. For each detected file, an asset will be created in the default \"Incoming\" nebula folder (ID 12).</p> <pre><code>&lt;service&gt;\n    &lt;folder id_storage=\"1\" path=\"media.dir/movies\"/&gt;\n    &lt;folder id_storage=\"2\" path=\"media.dir/episodes\"/&gt;\n&lt;/service&gt;\n</code></pre>"},{"location":"setup/watchfolders/#customizing-watch-folder-behavior","title":"Customizing watch folder behavior","text":"<p>The behavior of each watch folder can be customized using the following attributes within the <code>&lt;folder&gt;</code> tag.</p> Attribute Type Default Description id_storage required - Nebula storage id. path required - The path to the directory that will be monitored. id_folder optional 12 The ID of the Nebula folder where new assets will be created. recursive optional TRUE If true, the watch folder will scan subdirectories. hidden optional FALSE If true, files and folders beginning with a . will be ignored. quarantine_time optional 10 The number of seconds a file must remain unchanged before it is processed. This prevents incomplete file transfers from being imported. case_sensitive_exts optional FALSE If true, file extensions will be matched with case sensitivity."},{"location":"setup/watchfolders/#advanced-scripting","title":"Advanced scripting","text":"<p>For more advanced workflows, you can execute a Python script to modify the metadata of a newly created asset.  This is useful for tasks like loading metadata from a sidecar file or generating a custom asset identifier.</p> <p>The script is defined within a <code>&lt;post&gt;</code> tag inside the <code>&lt;folder&gt;</code> tag.  The newly created asset is available as the asset variable.</p>"},{"location":"setup/watchfolders/#example-creating-a-custom-identifier","title":"Example: Creating a Custom Identifier","text":"<p>This example uses the <code>shortuuid</code> library to generate a unique,  short identifier for the asset's <code>id/main</code> metadata field.</p> <pre><code>&lt;service&gt;\n  &lt;folder id_storage=\"1\" path=\"media.dir\" recursive=\"1\" id_folder=\"1\"&gt;\n    &lt;post&gt;\n&lt;![CDATA[\nimport shortuuid\nasset[\"id/main\"] = shortuuid.uuid()\n]]&gt;\n    &lt;/post&gt;\n  &lt;/folder&gt;\n&lt;/service&gt;\n</code></pre>"}]}